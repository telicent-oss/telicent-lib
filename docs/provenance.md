# Provenance and Audit

## Heartbeats

When a telicent-lib component is run, by default, it will automatically send a regular heartbeat messages to the provenance topic
using a `KafkaSink`.

The provenance topic can be specified with the environment variable `HEART_BEAT_PROVENANCE_TOPIC`. By default, heartbeats are 
written to the topic `provenance.live`.

Heartbeats are sent at a specified interval, by default every 15 seconds. This frequency can be configured per action and is detailed below.

### Message Format

**Message**

| Status           | Type         | Definition                                                                                                                             |
|------------------|--------------|----------------------------------------------------------------------------------------------------------------------------------------|
| id               | string       | An ID for the component, set by the user or generated by the component itself. Should be the same ID for all instances of a component. |
| instance_id      | string       | An ID for a specific instance of a component. By default UUID but could be overriden per instance if required.                         |
| name             | string       | The name of the component as defined by the user.                                                                                      |
| timestamp        | XSD datetime | Timestamp set by the component when sending the message.                                                                               |
| component_type   | string       | The type of component, e.g. adaptor, mapper, or projector.                                                                             | 
| reporting_period | integer      | The reporting period in seconds.                                                                                                       | 
| input            | JSON object  | Object describing the input of the component.                                                                                          | 
| output           | JSON object  | Object describing the output of the component.                                                                                         |
| status           | string       | The current status of the component, or REGISTERED if the component has just initialised.                                              | 

**Input / Output Object**

| Status | Type         | Definition                                                                  |
|--------|--------------|-----------------------------------------------------------------------------|
| name   | string       | The name of the input or output, e.g. the topic name, AWS bucket, database. |
| type   | string       | The type of input or output, e.g. topic, database, bucket.                  |

**Statuses**

| Status     | Definition                                                                      |
|------------|---------------------------------------------------------------------------------|
| STARTED    | A component has started and is ready to process messages.                       |
| RUNNING    | A component is running and has processed records. This is an on-going ping.     |
| COMPLETED  | A component has a fixed number of records and has processed all records.        |
| TERMINATED | A component has terminated, either through an interrupt, signal or due to error. |


### Disable the Reporter

```python
from telicent_lib.mapper import Mapper

mapper = Mapper(
    source=my_source, target=my_sink, map_function=map_func, has_reporter=False
)
mapper.run()
```

### Manually set the Reporter's sink

```python
from telicent_lib.mapper import Mapper
from telicent_lib.sinks.listSink import ListSink

reporter_sink = ListSink()
mapper = Mapper(
    source=my_source, target=my_sink, map_function=map_func, reporter_sink=reporter_sink
)
mapper.run()
```

### Changing the Reporting Frequency

```python
from telicent_lib.mapper import Mapper

mapper = Mapper(
    source=my_source, target=my_sink, map_function=map_func
)
mapper.reporter.heartbeat_time = 60  # set to every 60 seconds
mapper.run()
```


## Audit Headers

telicent-lib automatically applies headers to each record written by an adapter or a mapper. These records provide an audit that gives
each record a unique ID, which component wrote the record, and in the case of a mapper, the unique ID of the source record.

| Header             | Usage                                                                                                                            |
|--------------------|----------------------------------------------------------------------------------------------------------------------------------|
| `Request-Id`       | Unique ID for the request composed of the topic and a UUID                                                                       |
| `Exec-Path`        | ID of the component processing the record                                                                                        |
| `Input-Request-Id` | Input record's request ID, where present. Header may be repeated if there are multiple input requests, e.g. when merging records |
| `Data-Source-Name` | The name of the data source, as provided to the Action                                                                           |
| `Data-Source-Type` | The data source's type, as provided by the Action                                                                                |


## Data Catalog

By default, [Adapters and AutomaticAdapters](adapters.md) provide a method to notify a data catalog that a data source has been updated.

### Registering a Dataset

```python
...
from telicent_lib import Adapter, SimpleDataSet
from telicent_lib.adapter import KafkaSink
sink = KafkaSink(topic="raw-in")
dataset = SimpleDataSet(id="my-id", title="My Adapter", source_mime_type='text/csv')
adapter = Adapter(target=sink, dataset=dataset)
adapter.register_data_catalog()
```

The above will create a message on a topic (default: "catalog") with the following body:

```json
{
	"id": "telicent_lib.adapter",
	"title": "telicent_lib.adapter",
	"source_mime_type": "unknown"
}
```

| Status           | Type   | Definition                |
|------------------|--------|---------------------------|
| id               | string | The dataset's id.         |
| title            | string | The dataset's title.      |
| source_mime_type | string | The source's mime-type.   |

Additional data may be provided to the `register_data_catalog()` method which will be included in the output message.

```python
registration_fields = {'author':  'John Doe'}
adapter.register_data_catalog()
```

```json
{
	"id": "telicent_lib.adapter",
	"title": "telicent_lib.adapter",
	"source_mime_type": "unknown",
    "author":  "John Doe"
}
```

### Updating a Dataset

```python
from telicent_lib import Adapter, SimpleDataSet
from telicent_lib.adapter import KafkaSink
sink = KafkaSink(topic="raw-in")
dataset = SimpleDataSet(id="my-id", title="My Adapter", source_mime_type='text/csv')
adapter = Adapter(target=sink, name="Adapter", dataset=dataset)
adapter.update_data_catalog()
```

The above will create a message on a topic (default: "catalog") with the following body:

```json
{
	"id": "<module name>",
	"last_updated_at": "2000-01-01T00:00:00+00:00"
}
```
The `last_updated_at` timestamp would be the current system time when the message was created.

| Status          | Type         | Definition            |
|-----------------|--------------|-----------------------|
| is              | string       | The dataset's id.     |
| last_updated_at | XSD datetime | The current datetime. |


### Disable the data catalog sink

```python
adapter = Adapter(target=sink, name="Adapter", has_data_catalog=False)
```

The `update_data_catalog` method will not write to the sink and will log a warning when `has_data_catalog` is set to False.

### Specifying a custom data catalog target

By default, the data catalog notifications will be sent to a topic called "catalog". This may be overridden by specifying
a configuration value `DATA_CATALOG_TOPIC`.

Alternatively, where more fine-grained control over the target sink is required, you may initialise your own sink.

```python
kafka_config = {
    ...  # a custom Kafka config
}
dc_sink = KafkaSink(topic="my-topic", kafka_config=kafka_config)
adapter = Adapter(target=sink, name="Adapter", data_catalog_sink=dc_sink)
```

### Specifying headers for the data catalog record

Headers may be provided to the `update_data_catalog` and `register_data_catalog` methods.

```python
from telicent_lib import RecordUtils
headers = {"header-key": "header value"}
adapter.register_data_catalog(headers=RecordUtils.to_headers(headers))
adapter.update_data_catalog(headers=RecordUtils.to_headers(headers))
```


### DCATDataSet

telicent-lib also provides a `DCATDataSet` class. It can be used in the same way as `SimpleDataSet`, but its output
is RDF Turtle instead of JSON. `DCATDataSet` also requires certain keys be present in `registration_fields`.

- description
- publication_datetime
- publisher_id
- publisher_name
- publisher_email
- owner_id
- rights_title
- rights_description
- distribution_title
- distribution_id

```python
from telicent_lib import DCATDataSet

dataset = DCATDataSet(dataset_id='my-data-set', title='myfile.csv', source_mime_type='text/csv')
adapter = AutomaticAdapter(target=sink, dataset=dataset)
registration_fields = {
    'description': "Dataset's description",
    'publication_datetime': "2000-01-01T07:00:00+00:00",
    'publisher_id': "COMPANY-Org",
    'publisher_name': "Mr Owner",
    'publisher_email': "owner@example.com",
    'owner_id': "Data Owner",
    'rights_title': "Copyright 2000",
    'rights_description': "All rights reserved",
    'distribution_title': "My Pipeline Distribution",
    'distribution_id': "14343-232-90019"
}
adapter.register_data_catalog(registration_fields=registration_fields)
```

Produces the following turtle:

```
@prefix dcat: <http://www.w3.org/ns/dcat#> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix prov: <http://www.w3.org/ns/prov#> .
@prefix schema: <https://schema.org/> .
@prefix tcat: <http://telicent.io/catalog#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

tcat:my-data-set_dataset a dcat:Dataset ;
    dcterms:description "Dataset's description" ;
    dcterms:identifier "my-data-set" ;
    dcterms:issued "2000-01-01T07:00:00+00:00"^^xsd:dateTime ;
    dcterms:publisher tcat:COMPANY-Org ;
    dcterms:rights [ dcterms:description "All rights reserved" ;
            dcterms:title "Copyright 2000" ] ;
    dcterms:title "myfile.csv"@en ;
    dcat:distribution tcat:my-data-set_distribution ;
    prov:qualifiedAttribution [ a prov:Attribution ;
            dcat:hadRole <http://standards.iso.org/iso/19115/resources/Codelist/cat/codelists.xml#CI_RoleCode/owner> ;
            prov:agent "Data Owner" ] .

tcat:COMPANY-Org schema:email <owner@example.com> ;
    schema:name "Mr Owner"@en .

tcat:my-data-set_distribution a dcat:Distribution ;
    dcterms:identifier "14343-232-90019" ;
    dcterms:title "My Pipeline Distribution"@en ;
    dcat:mediaType <http://www.iana.org/assignments/media-types/text/csv> .
```

```python
from telicent_lib import DCATDataSet

dataset = DCATDataSet(dataset_id='my-data-set', title='myfile.csv', source_mime_type='text/csv')
adapter = AutomaticAdapter(target=sink, dataset=dataset)
adapter.update_data_catalog()
```

Produces the following turtle:

```
@prefix dcat: <http://www.w3.org/ns/dcat#> .
@prefix dcterms: <http://purl.org/dc/terms/> .
@prefix tcat: <http://telicent.io/catalog#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

tcat:my-data-set_dataset a dcat:Dataset ;
    dcterms:modified "2000-01-01T00:00:00+00:00"^^xsd:dateTime .
```
